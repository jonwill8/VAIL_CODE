# Introduction
___
This repository holds all my code/ideas/notes from VAIL Week 2 Assignments
___
# Responses
___
## Reflection - 2/16/21

### Question 1
    I think ML concepts were used pretty accurately in this game. An ML model was created to automate a task humans saw 
    fit for automation (hiring from many qualified canidates in a short amount of time). The hiring ML model was then trained
    on a massive dataset of (CV,hiring-outucome-result) from candiates who applied to work at both our firm and apple
    (the ml model needed more data for training purposes). The fact that the ML model amplified the biases both within 
    our firm's hiring process and within apple hiring process is extremely realistic. If a model is feed biased data,
    the model will inevitably spit out biased results (helping to perpetrate bias).

### Question 2
    I heard a podcast a couple of years back about an AI system in South FL which would predict if a criminal 
    would relaspe into criminality after being released. From my hazy memory, I recall while the model had high overall
    accuracy, it was dramatically overpredicting the ciminality relaspe rate for black criminals and dramatically 
    underpredicting the ciminality relaspe rate for white criminals (if analyzed with a confusion matrix, the 
    low specificity of the model is apparent).

    Unfortunately, I believe this model cannot be made more equitable. All criminal data is drawn from policing data. 
    Policing systems have been proven time and time again to hold bias against minority communities 
    (ex: disparities in arrest rates betwen black/white neighborhoods). Any criminal data feed into this model contains 
    intrinsic bias, thus the final model will spit our biased results. I selected this particular AI model becuase it
    was implemented in the area I live (South FL).
    
    

