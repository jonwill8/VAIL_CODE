{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# This notebook corresponds to the MNIST Day 11 Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import statistics as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras as ke\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "#importing NMIST data\n",
    "from keras.datasets import mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "#splitting into train/validation data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, train_size = 0.15, random_state=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K-Fold Cross Validation\n",
    "    * While our traditional approach of just splitting a macro Dataset into test/training data is fine,\n",
    "    it is more accurate to test our model with train data pulled from different subsets of the entire Dataset.\n",
    "    * K Fold CV splits a DS into various sub-bins (folds). Each fold is set as the test data once, while all other\n",
    "    folds are used to train the model (find optimal parameters).\n",
    "    *Model accuracy on the K sub-test datasets provides a more robust metric on overall model performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "### Showing some exploratory data plots"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Digit Distrbutions\n",
    "plt.figure(figsize=(15,7))\n",
    "g = sns.countplot(Y_train, palette=\"icefire\")\n",
    "plt.title(\"Number of digit classes\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#example image from x_train/ Dataset\n",
    "\n",
    "#pulling 0th x image array from x_train\n",
    "x_img_arr = X_train[0,:]\n",
    "#configuring plot\n",
    "plt.imshow(x_img_arr,cmap='gray')\n",
    "#titling w/ the image's respective #\n",
    "plt.title(f'{Y_train[0]}')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### As per the kaggle notebook, we must normalize+reshape+label encode data for easier model training\n",
    "\n",
    "#### Similar taking a z score in stats, this division by 255 will normalize our data and squish pixel values to be bound from 0-->1\n",
    "#### Pixel values in x arrays can only range from 0 --> 255, therefore dividing x arrays through by 255 will make nice data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#dividing through by 255\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#reshaping x data so it is of the form (# of images,image width,image height,ColorDimension)\n",
    "\"\"\"\n",
    "Note:\n",
    "ColorDimension = the # of data components associated with the pixels which give rise to our image.\n",
    "RGB images have a CD of 3 (each pixel contains data for Red,Green,Blue)\n",
    "By Contrast, Grayscale images have a CD of 1 (each pixel only contains data for its degree of black)\n",
    "\"\"\"\n",
    "X_train = X_train.reshape(X_train.shape[0],28,28,1)\n",
    "X_test = X_test.reshape(X_test.shape[0],28,28,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### label encoding = converting non numeric data labels (such as tall/short) to numbers (such as 1/0)\n",
    "#### label encoding is critical because it allows our model to calculate loss duing Supervised Learning and perform GD\n",
    "#### in this case, y labels will be encoded w.r.t to the number they represent\n",
    "#### ex: if the y output number is 7, we will label encode such that the model recieves a y output vector:\n",
    "\n",
    "[[0],[0],[0],[0],[0],[0],[0],[1],[0],[0]]\n",
    "\n",
    "#### For each feed forward, we will compare the ith neuron's prediction vs. the ith position in our label encoded y array\n",
    "#### Very easy to directly calcualte associated log losses from each neuron in the output layer and backpropogate\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#label encoding y train & test data\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)\n",
    "#Y_train = Y_train.reshape(Y_train.shape[0],1,Y_train.shape[1])\n",
    "Y_test = to_categorical(Y_test, num_classes = 10)\n",
    "#Y_test = Y_test.reshape(Y_test.shape[0],1,Y_test.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data has now been standardized, reshpaed, & label encoded. Now we can import CNN models for fitting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#making model imports from keras\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our CNN will have 2 Conv layers, 2 Pool Layers. We will then flatten data & Feed into Fully Connected NN wtih layers (1 input, 1 output)\n",
    "#### We will perform 2 droupouts during forward propogation to combat overfitting\n",
    "\n",
    "##### conv --> pool --> [dropout regularization] conv --> pool --> [dropout regularization] Flatten Data --> Fully Connnected ANN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#creating a function to define a CNN model\n",
    "\n",
    "#I will be messing with:\n",
    "    #the dropout rate (convo layer)\n",
    "    #the dropout rate (fully-connected layer)\n",
    "    #Activation function used by the Convo/FC layers\n",
    "\n",
    "def gen_model(convo_dropout_rate,fc_nn_dropout_rate,hidden_layer_activiation_func):\n",
    "    model = Sequential()\n",
    "    #conv layer 1\n",
    "    model.add(Conv2D(filters = 8, kernel_size = (5,5),padding = 'Same',\n",
    "                     activation ='relu', input_shape = (28,28,1)))\n",
    "    #pooling layer 1\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    #first dropout regularization\n",
    "    model.add(Dropout(convo_dropout_rate))\n",
    "    #conv layer 2\n",
    "    model.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same',\n",
    "                     activation ='relu'))\n",
    "    #pooling layer 2\n",
    "    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    #second dropout regularization\n",
    "    model.add(Dropout(convo_dropout_rate))\n",
    "\n",
    "    #flattening model tensors before feeding into FC NN\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #we have 256 Neurons b/c we have flattend our pooled data matrix to be of length 256 (we need 1 neruon per input variable)\n",
    "    model.add(Dense(256, activation = \"relu\"))\n",
    "    #third dropout regularization\n",
    "    model.add(Dropout(0.5))\n",
    "    #output layer w/ 10 neurons to perform the digit classification\n",
    "    model.add(Dense(10, activation = \"softmax\")) #emply softmax b/c mulitonomial classification [0-->9]\n",
    "\n",
    "    #defining the GD optimizer for our model\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    #defining our multinomial cost function\n",
    "    model.compile(optimizer = optimizer , loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#as another stall against overfitting we will inject some noise into our images\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # dimesion reduction\n",
    "        rotation_range=5,  # randomly rotate images in the range 5 degrees\n",
    "        zoom_range = 0.1, # Randomly zoom image 10%\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally 10%\n",
    "        height_shift_range=0.1,  # randomly shift images vertically 10%\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)\n",
    "\n",
    "datagen.fit(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#this method trains the passed model in more robust manner through K-Fold CV w/ 5 Folds\n",
    "# fun tweak parameter = # of epochs we train for\n",
    "# to save computing power, all models will be trained w/ a constant batch size (200)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def k_fold_train(epoch_num,convo_dropout_rate,fc_nn_dropout_rate,hidden_layer_activiation_func):\n",
    "    #init list of scores and model fit logs during sub-training\n",
    "    scores, histories = [], []\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    for train_ix, test_ix in kfold.split(X_train):\n",
    "        #spltting our macro train data into k subsets\n",
    "        xtrain, xtest = X_train[train_ix], X_train[test_ix]\n",
    "        ytrain, ytest = Y_train[train_ix], Y_train[test_ix]\n",
    "        #fitting model and pulling history\n",
    "        model = gen_model(convo_dropout_rate,fc_nn_dropout_rate,hidden_layer_activiation_func)\n",
    "        history  = model.fit(datagen.flow(xtrain,ytrain, batch_size=200),\n",
    "                              epochs = epoch_num, validation_data = (xtest,ytest), steps_per_epoch=xtrain.shape[0] //200)\n",
    "        #pulling model accuracy on the subtrain data\n",
    "        _, acc = model.evaluate(xtest, ytest, verbose=0)\n",
    "        print(f'Model accuracy during current subfold: {round(acc*100,4)}%')\n",
    "        #storing accuracies/histories for model metadata analysis\n",
    "        scores.append(acc)\n",
    "        histories.append(history)\n",
    "\n",
    "    return scores, histories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def summarize_performance(scores):\n",
    "\t# print summary\n",
    "\tprint(f'Accuracy Metrics: mean= {round(stats.mean(scores),4)} | std = {stats.stdev(scores)} | n={len(scores)} 3f std=%.3f, n=%d')\n",
    "\t# box and whisker plots of results\n",
    "\tpyplot.boxplot(scores)\n",
    "\tpyplot.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def summarize_diagnostics(histories):\n",
    "\tfor i in range(len(histories)):\n",
    "\t\t# plot loss\n",
    "\t\tplt.subplot(2, 1, 1)\n",
    "\t\tplt.title('Cross Entropy Loss')\n",
    "\t\tplt.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "\t\tplt.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "\t\t# plot accuracy\n",
    "\t\tplt.subplot(2, 1, 2)\n",
    "\t\tplt.title('Classification Accuracy')\n",
    "\t\tplt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "\t\tplt.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n",
    "\tplt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#init arrays of various values for our hyperparameters of interest\n",
    "\n",
    "convo_dropout_rates  = [0,0.25,0.5,0.75,1]\n",
    "fc_nn_dropout_rate = [0,0.25,0.5,0.75,1]\n",
    "hidden_layer_activiation_func = ['logistic','relu','tanh']\n",
    "\n",
    "\n",
    "mean_scores_log = [] #log of model average under different hyperparams\n",
    "histories_dict = {} #meanscore,history obj\n",
    "\n",
    "#this function is used to plot the optimal 3 models from the above set of hyperparameters\n",
    "for convo_drop_rate in convo_dropout_rates:\n",
    "    for fc_nn_dop_rate in fc_nn_dropout_rate:\n",
    "        for act_func in hidden_layer_activiation_func:\n",
    "            scores,histories =  k_fold_train(epoch_num=5,convo_dropout_rate=convo_drop_rate,fc_nn_dropout_rate=fc_nn_dop_rate,\n",
    "                                             hidden_layer_activiation_func=act_func)\n",
    "            mean_score = stats.mean(scores)\n",
    "            if len(mean_scores_log)<3:\n",
    "                mean_scores_log.append(mean_score) #adding w/o discrimination until we have 3 scores arrays\n",
    "                histories_dict[mean_score] = histories\n",
    "            else:\n",
    "                if mean_score>min(mean_scores_log):\n",
    "                    #deleting the lowest score from our mean score log\n",
    "                    min_mean_score = min(mean_scores_log)\n",
    "                    del mean_scores_log[mean_scores_log.index(min_mean_score)]\n",
    "                    mean_scores_log.append(mean_score)\n",
    "                    #adding the new higher score to the dict and removing the older lower score\n",
    "                    histories_dict[mean_score] = histories\n",
    "                    histories_dict.pop(min_mean_score)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "#plotting the metrics of the 3 best models:\n",
    "for score in mean_scores_log:\n",
    "    print(f'This model had a mean accuracy score of {score}')\n",
    "    summarize_diagnostics(histories_dict[score])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}