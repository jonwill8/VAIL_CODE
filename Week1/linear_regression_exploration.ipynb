{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook Explores the Gradient Descent Process Which Underlies Linear Regression\n",
    "\n",
    "Example Data Pulled from this [Article](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Making our necessary imports to read/process the data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#reading the input data using into a pandas dataframe\n",
    "data = pd.read_csv('Linear_Regression_Data.csv')\n",
    "\n",
    "#pulling our x and y values from the data frame\n",
    "x_vals = data['X Data']\n",
    "y_vals = data['Y Data']\n",
    "\n",
    "#plotting our x/y data\n",
    "plt.xlabel('Independent X Values')\n",
    "plt.ylabel('Dependent Y Values')\n",
    "plt.title('Scatter Plot of our Regression Data')\n",
    "plt.scatter(x_vals,y_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Note:\n",
    "* Our Linear Regression Model is of the form: $\\hat{y} = mx + c$\n",
    "\n",
    "* $\\hat{y}$ = our prediction given the independent variable x and parameters m & c\n",
    "\n",
    "* The parameters m & c will be fitted using Gradient Descent\n",
    "\n",
    "* We will fit our model for 1000 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Laying Linear Regression Model Groundwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#hard coding a learning rate for our model\n",
    "learn_rate = 0.0001\n",
    "\n",
    "#hard coding the number of epochs we will train our model\n",
    "epochs = 1000\n",
    "\n",
    "\"\"\"\n",
    "Initializing logs of:\n",
    "1) m & c parameter values --> Will converge to optimized values\n",
    "2) Overall Error --> Will minimize\n",
    "3) Partial m & c Derivative Values --> Will converge to 0\n",
    "\"\"\"\n",
    "\n",
    "m_log = [0]*epochs\n",
    "c_log = [0]*epochs\n",
    "partial_m_log = [0]*epochs\n",
    "partial_c_log = [0]*epochs\n",
    "error_log = [0]*epochs\n",
    "\n",
    "#initilizing m & c parameters to 0\n",
    "m = 0\n",
    "c = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Steps of the Iterative Gradient Descent Process:\n",
    "1. Compute our current predictor function given m & c parameters. Recall: $\\hat{y} = mx + c$\n",
    "2. Calculate the partial derivatives of our error function using our predictor function + the current m & c parameter values\n",
    "3. Update our m & c parameters via: $parameter_{next} = parameter_{current} - (LearningRate)(\\frac{\\partial Error}{\\partial parameter_{current}})$\n",
    "4. Repeat Steps 1-3 for our number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Creating our Necessary Helper Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function calculates the output value of our predictor function given values for parameters m/c & an x input datapoint\n",
    "\n",
    "Recall, our predictor function is of the form: $\\hat{y} = mx + c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predictor_function(m,c,x):\n",
    "    \"\"\"\n",
    "    :param m --> The current Value of our m parameter\n",
    "    :param c --> The current Value of our c parameter\n",
    "    :param x --> The independent X value\n",
    "    :return --> Our current Y prediction for the provided x value\n",
    "    \"\"\"\n",
    "    return m*x + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function calculates the partial m derivative of our Error function given the current m & c parameter values.\n",
    "\n",
    "$\\frac{\\partial Error}{\\partial m}=\\frac{-2}{n}\\sum\\limits_{i=0}^{n}(x_i)(y_i-\\hat{y}_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_partial_m(x_data,y_data,m_current,c_current):\n",
    "    \"\"\"\n",
    "    :param m_current --> the current Value of our m parameter\n",
    "    :param c_current --> the current Value of our c parameter\n",
    "    :param x_data --> The x training data we our fitting our model to the\n",
    "    :param y_data --> The y training data we our fitting our model to the\n",
    "    return --> The Partial m Derivative of our Error function given our current m & c parameter values\n",
    "    \"\"\"\n",
    "    return -2/len(x_data) * sum([x_data*(y_data - predictor_function(m_current,c_current,x_data))\n",
    "                       for x_data,y_data in zip(x_data,y_data)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function calculates the partial c derivative of our Error function given the current m & c parameter values.\n",
    "\n",
    "$\\frac{\\partial Error}{\\partial c}=\\frac{-2}{n}\\sum\\limits_{i=0}^{n}(y_i-\\hat{y}_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_partial_c(x_data,y_data,m_current,c_current):\n",
    "    \"\"\"\n",
    "    :param x_data --> The x training data we our fitting our model to the\n",
    "    :param y_data --> The y training data we our fitting our model to the\n",
    "    :param m_current --> the current Value of our m parameter\n",
    "    :param c_current --> the current Value of our c parameter\n",
    "    return --> The Partial c Derivative of our Error function given our current m & c parameter values\n",
    "    \"\"\"\n",
    "    return -2/len(x_data) * sum([y_data - predictor_function(m_current,c_current,x_data)\n",
    "                                 for x_data,y_data in zip(x_data,y_data)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function is used to iteratively update our m parameter until it is optimized.\n",
    "\n",
    "$m_{next} = m_{current} - (LearningRate)(\\frac{\\partial Error}{\\partial m_{current}})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_m(m_current,learning_rate,partial_m):\n",
    "    \"\"\"\n",
    "    :param m_current --> the current Value of our m parameter\n",
    "    :param learning_rate --> the step size adjustor for our gradient descent optimization\n",
    "    :param partial_m --> the current partial derivative of our cost function with respect to m\n",
    "    return --> Our updated m parameter value\n",
    "    \"\"\"\n",
    "    return m_current - learning_rate*partial_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function is used to iteratively update our c parameter until it is optimized.\n",
    "\n",
    "$c_{next} = c_{current} - (LearningRate)(\\frac{\\partial Error}{\\partial c_{current}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_c(c_current,learning_rate,partial_c):\n",
    "    \"\"\"\n",
    "    :param c_current --> the current Value of our c parameter\n",
    "    :param learning_rate --> the step size adjustor for our gradient descent optimization\n",
    "    :param partial_c --> the current partial derivative of our cost function with respect to c\n",
    "    return --> Our updated c parameter value\n",
    "    \"\"\"\n",
    "    return c_current - learning_rate*partial_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function is used to compute the value of our error function given the current values of the m & c parameters\n",
    "\n",
    "$Error(m,c)=\\frac{1}{n}\\sum\\limits_{i=0}^{n}(y_i-\\hat{y_i})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def error_function(x_data,y_data,m_current,c_current):\n",
    "    \"\"\"\n",
    "    :param x_data --> The x training data we our fitting our model to\n",
    "    :param y_data --> The y training data we our fitting our model to\n",
    "    :param m_current --> the current Value of our m parameter\n",
    "    :param c_current --> the current Value of our c parameter\n",
    "    return --> The Mean Squared Error at our current m & c parameter values\n",
    "    \"\"\"\n",
    "\n",
    "    return 1/len(x_data) * sum([(y_data - predictor_function(m_current,c_current,x_data))**2\n",
    "                                for x_data,y_data in zip(x_data,y_data)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Performing Iterative Gradient Descent to Optimize Our Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    #calculating and logging our current partial m value\n",
    "    partial_m = calc_partial_m(x_vals,y_vals,m,c)\n",
    "    partial_m_log[epoch] = partial_m\n",
    "\n",
    "    #calculating and logging our current partial c value\n",
    "    partial_c = calc_partial_c(x_vals,y_vals,m,c)\n",
    "    partial_c_log[epoch] = partial_c\n",
    "\n",
    "    #calculating and logging our current m parameter value\n",
    "    m = update_m(m,learn_rate,partial_m)\n",
    "    m_log[epoch] = m\n",
    "\n",
    "    #calculating and logging our current c parameter value\n",
    "    c = update_c(c,learn_rate,partial_c)\n",
    "    c_log[epoch] = c\n",
    "\n",
    "    #calculating and logging our current error value\n",
    "    error = error_function(x_vals,y_vals,m,c)\n",
    "    error_log[epoch] = error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plotting our m parameter value over the number epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs),m_log)\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.ylabel('M Parameter')\n",
    "plt.title(f'M parameter converges to optimized value over {epochs} training epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plotting our c parameter value over the number epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs),c_log)\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.ylabel('C Parameter')\n",
    "plt.title(f'C parameter converges to optimized value over {epochs} training epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting our partial m value over the number epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs),partial_m_log)\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Partial M Value')\n",
    "plt.title(f'Our partial M Derivative converges to 0 over {epochs} training epochs')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plotting our partial c value over the number epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(epochs),partial_c_log)\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.ylabel('Partial C Value')\n",
    "plt.title(f'Our partial C Derivative converges to 0 over {epochs} training epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plotting our overall Mean Squared Error value over the number epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#plotting our overall error value over our number epochs\n",
    "plt.plot(np.arange(epochs),error_log)\n",
    "plt.xlabel(\"Training Epoch\")\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title(f'Our Mean Squared Error Minimizes over {epochs} training epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plotting our optimized linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#pulling the optimized m/c parameter values from our very last epochs\n",
    "m_optimized = m_log[-1]\n",
    "c_optimized = c_log[-1]\n",
    "\n",
    "y_predicts = [predictor_function(m_optimized,c_optimized,x_val) for x_val in x_vals]\n",
    "\n",
    "#plotting our raw data\n",
    "plt.scatter(x_vals,y_vals)\n",
    "\n",
    "#plotting our prediction line\n",
    "plt.plot(x_vals,y_predicts)\n",
    "\n",
    "#Labeling Plot\n",
    "plt.title(f'Optimized Linear Regression Model has a Mean Squared Error of '\n",
    "          f'{round(error_function(x_vals,y_vals,m_optimized,c_optimized),4)}')\n",
    "#printing our optimized parameter\n",
    "print(f'Optimized M Parameter Value: {m_optimized}')\n",
    "print(f'Optimized C Parameter Value: {c_optimized}')\n",
    "print(f'Overall Final Predictor Function: Y = {round(m_optimized,4)}x + {round(c_optimized,4)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}