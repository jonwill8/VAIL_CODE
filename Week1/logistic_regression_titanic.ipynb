{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This Script Explores Using Stochastic Gradient Descent For Multivariate Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression Recap\n",
    "\n",
    "### Predictor Function:\n",
    "\n",
    "* $\\hat{y}(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "    * Where $z = b + w_1x_1 + w_2x_2 + ... w_nx_n$\n",
    "\n",
    "Because this data set has 7 input variables, our z term becomes:\n",
    "* $z = b + w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + w_5x_5 + + w_6x_6 + w_7x_7$\n",
    "\n",
    "We can represent the repeated $w_nx_n$ term in our z term by a matrix multiplication:\n",
    "\n",
    "* $z = b + W^TX$ (where W is a row vector of our weights and X is a row vector of our input variables)\n",
    "\n",
    "### Cost Function:\n",
    "\n",
    "Since we will be performing Stochastic Gradient Descent, Out Single Sample Cost Function is:\n",
    "\n",
    "* $ Error =  -y_i \\ln(\\hat{y}_i) - (1-y_i) \\ln(1-\\hat{y}_i) $\n",
    "\n",
    "The partial derivative of this cost function for each sample w.r.t each weight is:\n",
    "\n",
    "* $\\frac{\\partial Error}{\\partial w_n} (\\hat{y_i}-y_i)x_n$\n",
    "\n",
    "The partial derivative of this cost function for each sample w.r.t the bias term is:\n",
    "\n",
    "* $\\frac{\\partial Error}{\\partial b} (\\hat{y_i}-y_i)$\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "When performing SGD, we will compute the partial cost for all weights & the bias term after pulling each\n",
    "sample from the training data. We update each parameter by:\n",
    "\n",
    "* $parameter_{new} = parameter_{old} - LearningRate*\\frac{\\partial Error}{\\partial parameter_{old}}$\n",
    "\n",
    "Parameters are updated n number of times each epoch where n is the size of our # of training samples\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.      0.     34.5    ...  7.8292  2.      0.    ]\n",
      " [ 3.      1.     47.     ...  7.      1.      1.    ]\n",
      " [ 2.      0.     62.     ...  9.6875  2.      0.    ]\n",
      " ...\n",
      " [ 3.      0.     38.5    ...  7.25    1.      0.    ]\n",
      " [ 3.      0.         nan ...  8.05    1.      0.    ]\n",
      " [ 3.      0.         nan ... 22.3583  0.      0.    ]]\n"
     ]
    }
   ],
   "source": [
    "#making necessary imports & pulling data from csv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "x_df = pd.read_csv('test_X_titanic.csv')\n",
    "y_df = pd.read_csv('test_Y_titanic.csv')\n",
    "\n",
    "x_train_data = x_df.values[:,1:]\n",
    "y_train_data = y_df.values[:,1:]\n",
    "\n",
    "#print(x_train_data)\n",
    "#print(y_train_data)\n",
    "\n",
    "#combining the training data arrays into 1 np array\n",
    "train_data = np.append(x_train_data, y_train_data, axis=1)\n",
    "\n",
    "print(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def predictor_func(bias,weight_vec,input_var_vec):\n",
    "    \"\"\"\n",
    "    This function returns our logistic model prediction provided our bias term, and vectors for our weights & their associated input variables\n",
    "    \"\"\"\n",
    "    z = bias + np.matmul(weight_vec.T,input_var_vec)\n",
    "    return 1/(1+np.exp(-z))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def partial_error_wrt_weights(y_observation,bias,weight_vec,input_var_vec):\n",
    "    \"\"\"\n",
    "    This function returns the partial error derivative w.r.t each weight in our weight vector\n",
    "    \"\"\"\n",
    "    #pulling our y prediction on the passed input variable vector\n",
    "    y_prediction = predictor_func(bias,weight_vec,input_var_vec)\n",
    "    #returning an np array of the partial associated for each weight\n",
    "    return np.array([(y_prediction - y_observation)*input_var for input_var in input_var_vec])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def partial_error_wrt_bias(y_observation,bias,weight_vec,input_var_vec):\n",
    "    \"\"\"\n",
    "    This function returns the partial error derivative w.r.t to the bias term\n",
    "    \"\"\"\n",
    "    return predictor_func(bias,weight_vec,input_var_vec) - y_observation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def update_params(y_observation,bias,weight_vec,input_var_vec,learn_rate):\n",
    "    \"\"\"\n",
    "    This function updates our weight parameter and bias term\n",
    "    \"\"\"\n",
    "    #updating weights\n",
    "    weight_vec_new = weight_vec - learn_rate*partial_error_wrt_weights(y_observation,bias,weight_vec,input_var_vec)\n",
    "    bias_new = bias - learn_rate*partial_error_wrt_bias(y_observation,bias,weight_vec,input_var_vec)\n",
    "    return weight_vec_new,bias_new"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def error_func(training_data,bias,weight_vec):\n",
    "    \"\"\"\n",
    "    This function computes our average error over all samples in the training data\n",
    "    sample[0:6] = the x input array\n",
    "    sample[-1] = the y observation\n",
    "    \"\"\"\n",
    "    return 1/len(train_data.shape[1])*sum([-data_sample[-1]*np.log(predictor_func(bias,weight_vec,data_sample[:,6])) -\n",
    "                                (1-data_sample[-1])*np.log(1-predictor_func(bias,weight_vec,data_sample[:,6])) for data_sample in training_data])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#performing the SGD\n",
    "\n",
    "#hard coding a learn_rate & epoch #\n",
    "learning_rate = 0.001\n",
    "epochs = 5000\n",
    "\n",
    "#randomly assigning weight/bias term values\n",
    "weights = np.array([np.random.randint(-1,1) for _ in range(x_train_data.shape[1])])\n",
    "b = 1\n",
    "\n",
    "#init logs of variables of interest\n",
    "\n",
    "#weights\n",
    "w1_log, w2_log, w3_log, w4_log, w5_log, w6_log, w7_log  = [0]*epochs,[0]*epochs,[0]*epochs,[0]*epochs,[0]*epochs,[0]*epochs,[0]*epochs\n",
    "#bias\n",
    "b_log = [0]*epochs\n",
    "#weight partials\n",
    "partial_w1_log,partial_w2_log,partial_w3_log,partial_w4_log,partial_w5_log,partial_w6_log,partial_w7_log = [0]*epochs,[0]*epochs,[0]*epochs,[0]*epochs,[0]*epochs,[0]*epochs,[0]*epochs\n",
    "#bias partial\n",
    "partial_b_log = [0]*epochs\n",
    "#error log\n",
    "error_log = [0]*epochs\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #randomly shuffling the training data\n",
    "    np.random.shuffle(train_data)\n",
    "    #iterating over each sample in training data and updating params\n",
    "    for sample in train_data:\n",
    "        #splitting our sample into the vector of x inputs and scalar y output\n",
    "        x_data,y_datapoint = sample[0:6] , sample[-1]\n",
    "        #finding partials for current train data sample\n",
    "        weight_partials = partial_error_wrt_weights(y_datapoint,b,weights,x_data)\n",
    "        bias_partial =  partial_error_wrt_bias(y_datapoint,b,weights,x_data)\n",
    "        #logging partial values\n",
    "        partial_w1_log[epoch] ,partial_w2_log[epoch] ,partial_w3_log[epoch],partial_w4_log[epoch],partial_w5_log[epoch],partial_w6_log[epoch],partial_w7_log[epoch]  = weight_partials\n",
    "        partial_b_log[epoch] = bias_partial\n",
    "        #updating our parameters\n",
    "        weights,b = update_params(y_datapoint,b,weights,x_data,learning_rate)\n",
    "        #logging updated parameter values\n",
    "        w1_log[epoch], w2_log[epoch], w3_log[epoch], w4_log[epoch], w5_log[epoch], w6_log[epoch], w7_log[epoch] = weights\n",
    "        b_log[epoch] = b\n",
    "    #calculating & logging our overall Log Loss Error after each training epoch\n",
    "    error_log[epoch] = error_func(train_data,b,weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting weight 1 value vs # of epochs\n",
    "plt.plot(np.arange(epochs),w1_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Weight 1 Value')\n",
    "plt.title('Weight 1 Converges to its Optimal Value')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting weight 2 value vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),w2_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Weight 2 Value')\n",
    "plt.title('Weight 2 Converges to its Optimal Value')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting weight 3 value vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),w3_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Weight 3 Value')\n",
    "plt.title('Weight 3 Converges to its Optimal Value')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting weight 4 value vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),w4_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Weight 4 Value')\n",
    "plt.title('Weight 4 Converges to its Optimal Value')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting weight 5 value vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),w5_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Weight 5 Value')\n",
    "plt.title('Weight 5 Converges to its Optimal Value')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting weight 6 value vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),w6_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Weight 6 Value')\n",
    "plt.title('Weight 6 Converges to its Optimal Value')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting weight 7 value vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),w7_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Weight 7 Value')\n",
    "plt.title('Weight 7 Converges to its Optimal Value')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting Partial Derivative of Cost wrt Weight 1 vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),partial_w1_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Partial Cost wrt Weight 1')\n",
    "plt.title('Partial Cost wrt. Weight 1 Converges to 0')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting Partial Derivative of Cost wrt Weight 2 vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),partial_w2_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Partial Cost wrt Weight 2')\n",
    "plt.title('Partial Cost wrt. Weight 2 Converges to 0')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting Partial Derivative of Cost wrt Weight 3 vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),partial_w3_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Partial Cost wrt Weight 3')\n",
    "plt.title('Partial Cost wrt. Weight 3 Converges to 0')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting Partial Derivative of Cost wrt Weight 4 vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),partial_w4_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Partial Cost wrt Weight 4')\n",
    "plt.title('Partial Cost wrt. Weight 4 Converges to 0')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#plotting Partial Derivative of Cost wrt Weight 5 vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),partial_w5_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Partial Cost wrt Weight 5')\n",
    "plt.title('Partial Cost wrt. Weight 5 Converges to 0')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting Partial Derivative of Cost wrt Weight 6 vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),partial_w6_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Partial Cost wrt Weight 6')\n",
    "plt.title('Partial Cost wrt. Weight 6 Converges to 0')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting Partial Derivative of Cost wrt Weight 7 vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),partial_w7_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Partial Cost wrt Weight 7')\n",
    "plt.title('Partial Cost wrt. Weight 7 Converges to 0')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting bias term value vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),b_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Bias Term Value')\n",
    "plt.title('Bias Term Converges to its Optimal Value')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting Partial Derivative of Cost wrt Bias Term vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),partial_b_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Partial Cost wrt Bias Term')\n",
    "plt.title('Partial Cost wrt. Bias Term Converges to 0')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting Cost vs # of epochs\n",
    "\n",
    "plt.plot(np.arange(epochs),error_log)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Cost Value')\n",
    "plt.title('Total Cost Minimizes under Gradient Descent ')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#calculating model accuracy with optimal parameters\n",
    "opt_weights = w1_log[-1],w2_log[-1],w3_log[-1],w4_log[-1],w5_log[-1],w6_log[-1],w7_log[-1]\n",
    "opt_b = b_log[-1]\n",
    "#init counter of right predictions\n",
    "right_predicts = 0\n",
    "\n",
    "#Calculating our model accuracy against the training data\n",
    "for row_index in range(train_data.shape[0]):\n",
    "    observation_vec = row_index[row_index,:]\n",
    "    x_input_vec = observation_vec[0:6]\n",
    "    y_output = observation_vec[-1]\n",
    "    prediction = predictor_func(opt_b,opt_weights,x_input_vec)\n",
    "    if y_output == 1 and 0.5<=prediction<=1:\n",
    "        right_predicts+=1\n",
    "    elif y_output == 0 and 0<=prediction<0.5:\n",
    "        right_predicts+=1\n",
    "    else:\n",
    "        pass\n",
    "print(f'This Logistic Regression Model has an overall accuracy of {round(right_predicts/train_data.shape[0])*100,4}%')\n",
    "#printing optimized model params\n",
    "print(f'Optimized Model Params: {opt_weights}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}