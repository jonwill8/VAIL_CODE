{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/icml_face_data.csv\n/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/fer2013.tar.gz\n/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/example_submission.csv\n/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/train.csv\n/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/test.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imports\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\nfrom keras.layers import Conv2D,MaxPooling2D\nimport os\n\n# import imageio\nfrom matplotlib import image\n\n\nfrom sklearn.metrics import accuracy_score\n\n# import tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, Convolution2D, Flatten, MaxPooling2D, Reshape, InputLayer\nfrom keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.preprocessing.image import load_img","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#parsing data into train and validation:\ndata = pd.read_csv('/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/icml_face_data.csv')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"   emotion     Usage                                             pixels\n0        0  Training  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n1        0  Training  151 150 147 155 148 133 111 140 170 174 182 15...\n2        2  Training  231 212 156 164 174 138 161 173 182 200 106 38...\n3        4  Training  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n4        6  Training  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emotion</th>\n      <th>Usage</th>\n      <th>pixels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Training</td>\n      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Training</td>\n      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Training</td>\n      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Training</td>\n      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>Training</td>\n      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function parses data into train/val/test\ndef prepare_data(data):\n    \"\"\" Prepare data for modeling \n        input: data frame with labels und pixel data\n        output: image and label array \"\"\"\n    \n    image_array = np.zeros(shape=(len(data), 48, 48))\n    image_label = np.array(list(map(int, data['emotion'])))\n    \n    for i, row in enumerate(data.index):\n        image = np.fromstring(data.loc[row, ' pixels'], dtype=int, sep=' ')\n        image = np.reshape(image, (48, 48))\n        image_array[i] = image\n        \n    return image_array, image_label","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_array, train_image_label = prepare_data(data[data[' Usage']=='Training'])\nval_image_array, val_image_label = prepare_data(data[data[' Usage']=='PrivateTest'])\ntest_image_array, test_image_label = prepare_data(data[data[' Usage']=='PublicTest'])","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reshaping our image matrices & normalizing:\n\ntrain_images = train_image_array.reshape((train_image_array.shape[0], 48, 48, 1))\nX_train = train_images.astype('float32')/255\nval_images = val_image_array.reshape((val_image_array.shape[0], 48, 48, 1))\nX_val = val_images.astype('float32')/255\ntest_images = test_image_array.reshape((test_image_array.shape[0], 48, 48, 1))\nX_test = test_images.astype('float32')/255\n\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#hot encoding our y vectors:\ny_train = keras.utils.to_categorical(train_image_label)\ny_val = keras.utils.to_categorical(val_image_label)\ny_test = keras.utils.to_categorical(test_image_label)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a DF for y_train data:\npd.DataFrame(y_train)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"         0    1    2    3    4    5    6\n0      1.0  0.0  0.0  0.0  0.0  0.0  0.0\n1      1.0  0.0  0.0  0.0  0.0  0.0  0.0\n2      0.0  0.0  1.0  0.0  0.0  0.0  0.0\n3      0.0  0.0  0.0  0.0  1.0  0.0  0.0\n4      0.0  0.0  0.0  0.0  0.0  0.0  1.0\n...    ...  ...  ...  ...  ...  ...  ...\n28704  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n28705  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n28706  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n28707  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n28708  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n\n[28709 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>28704</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>28705</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>28706</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>28707</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>28708</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>28709 rows Ã— 7 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adding a data augmenter to bolster model accuracy on unseen images:\nfrom keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define vars\ninput_reshape = (48, 48, 1)\n\npool_size = (2, 2)\n\nhidden_num_units = 265\noutput_num_units = 7\n\nepochs = 10\nbatch_size = 128","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating Sequential CNN:\n\nmodel = Sequential([\n\nConvolution2D(75,(2,2), activation='relu',input_shape=input_reshape),\nMaxPooling2D((2,2)),\n\nConvolution2D(50,(2,2), activation='relu'),\nMaxPooling2D((2,2)),\n\nConvolution2D(25,(2,2), activation='relu'),\n\nFlatten(),\n\nDense(hidden_num_units, 'relu'),\n\nDense(output_num_units,'softmax'),\n ])\n","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing model summary:\nmodel.summary()","execution_count":14,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 47, 47, 75)        375       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 23, 23, 75)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 22, 22, 50)        15050     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 11, 11, 50)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 10, 10, 25)        5025      \n_________________________________________________________________\nflatten (Flatten)            (None, 2500)              0         \n_________________________________________________________________\ndense (Dense)                (None, 265)               662765    \n_________________________________________________________________\ndense_1 (Dense)              (None, 7)                 1862      \n=================================================================\nTotal params: 685,077\nTrainable params: 685,077\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\ntrained_model_conv = model.fit(X_train, y_train, epochs =epochs, batch_size=batch_size, validation_data=(X_val, y_val))","execution_count":15,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n225/225 [==============================] - 6s 12ms/step - loss: 1.7691 - accuracy: 0.2737 - val_loss: 1.5554 - val_accuracy: 0.3892\nEpoch 2/10\n225/225 [==============================] - 2s 10ms/step - loss: 1.5099 - accuracy: 0.4201 - val_loss: 1.4184 - val_accuracy: 0.4517\nEpoch 3/10\n225/225 [==============================] - 2s 9ms/step - loss: 1.3656 - accuracy: 0.4734 - val_loss: 1.3313 - val_accuracy: 0.4862\nEpoch 4/10\n225/225 [==============================] - 2s 9ms/step - loss: 1.2659 - accuracy: 0.5222 - val_loss: 1.2773 - val_accuracy: 0.5074\nEpoch 5/10\n225/225 [==============================] - 2s 9ms/step - loss: 1.1759 - accuracy: 0.5536 - val_loss: 1.2874 - val_accuracy: 0.5013\nEpoch 6/10\n225/225 [==============================] - 2s 9ms/step - loss: 1.1055 - accuracy: 0.5860 - val_loss: 1.2326 - val_accuracy: 0.5266\nEpoch 7/10\n225/225 [==============================] - 2s 10ms/step - loss: 1.0215 - accuracy: 0.6223 - val_loss: 1.2345 - val_accuracy: 0.5364\nEpoch 8/10\n225/225 [==============================] - 2s 9ms/step - loss: 0.9415 - accuracy: 0.6577 - val_loss: 1.2368 - val_accuracy: 0.5378\nEpoch 9/10\n225/225 [==============================] - 2s 9ms/step - loss: 0.8366 - accuracy: 0.6996 - val_loss: 1.2771 - val_accuracy: 0.5461\nEpoch 10/10\n225/225 [==============================] - 2s 9ms/step - loss: 0.7422 - accuracy: 0.7381 - val_loss: 1.3416 - val_accuracy: 0.5366\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting model loss curves:\nimport matplotlib.pyplot as plt\nplt.plot(trained_model_conv.history['loss'],label='Train Loss')\nplt.plot(trained_model_conv.history['val_loss'],label='Val Loss')\nplt.xlabel('Ephocs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#increasing model complexity by adding more convo layers:\nmodel = Sequential()\n#Block-1\nmodel.add(Convolution2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = input_reshape))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Convolution2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Dropout(0.2))\n\n#Block-2\nmodel.add(Convolution2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = input_reshape))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Convolution2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Dropout(0.2))\n\n#Block-3\nmodel.add(Convolution2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = input_reshape))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Convolution2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Dropout(0.2))\n\n#Block-4\nmodel.add(Convolution2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = input_reshape))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Convolution2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Dropout(0.2))\n\n#Block-5\nmodel.add(Flatten())\nmodel.add(Dense(units = 256 , activation = 'relu'))\nmodel.add(Dropout(0.3))\n\n#Block-6\nmodel.add(Dense(units = 7 , activation = 'softmax'))\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","execution_count":18,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 48, 48, 256)       2560      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 24, 24, 256)       0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 24, 24, 256)       590080    \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 12, 12, 256)       0         \n_________________________________________________________________\ndropout (Dropout)            (None, 12, 12, 256)       0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 12, 12, 128)       295040    \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 6, 6, 128)         0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 6, 6, 128)         147584    \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 3, 3, 128)         0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 3, 3, 128)         0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 3, 3, 75)          86475     \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 2, 2, 75)          0         \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 2, 2, 50)          33800     \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 1, 1, 50)          0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 1, 1, 50)          0         \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 1, 1, 64)          28864     \n_________________________________________________________________\nmax_pooling2d_8 (MaxPooling2 (None, 1, 1, 64)          0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 1, 1, 64)          36928     \n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 1, 1, 64)          0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 1, 1, 64)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 64)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 256)               16640     \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 7)                 1799      \n=================================================================\nTotal params: 1,239,770\nTrainable params: 1,239,770\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing regularization techniqhes from keras\nfrom keras.optimizers import RMSprop,SGD,Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('EmotionDetectionModel.h5',\n                             monitor='val_loss',\n                             mode='min',\n                             save_best_only=True,\n                             verbose=1)\nearlystop = EarlyStopping(monitor='val_loss',\n                          min_delta=0,\n                          patience=3,\n                          verbose=1,\n                          restore_best_weights=True\n                          )\nreduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                              factor=0.2,\n                              patience=3,\n                              verbose=1,\n                              min_delta=0.0001)\ncallbacks = [earlystop,checkpoint,reduce_lr]","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\ntrained_model_conv = model.fit(X_train, y_train, epochs =20, batch_size=128,callbacks=callbacks, validation_data=(X_val, y_val))","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/20\n225/225 [==============================] - 9s 35ms/step - loss: 1.8438 - accuracy: 0.2438 - val_loss: 1.8190 - val_accuracy: 0.2449\n\nEpoch 00001: val_loss improved from inf to 1.81904, saving model to EmotionDetectionModel.h5\nEpoch 2/20\n 65/225 [=======>......................] - ETA: 5s - loss: 1.8110 - accuracy: 0.2497","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#showing model loss curves:\n\nplt.plot(trained_model_conv.history['loss'],label='Train Loss')\nplt.plot(trained_model_conv.history['val_loss'],label='Val Loss')\nplt.xlabel('Ephocs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}